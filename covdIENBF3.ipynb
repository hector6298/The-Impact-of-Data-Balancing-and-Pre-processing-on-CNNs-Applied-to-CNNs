{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "covdIENBF3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hector6298/The-Impact-of-Data-Balancing-and-Pre-processing-on-CNNs-Applied-to-CNNs/blob/master/covdIENBF3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zeh2uQ2QMgi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "3b4e4d84-45eb-4547-d262-af58a3c8632e"
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv11DbxZ7CY6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "17026b3d-781d-4b8b-f99c-2c13ce57a45e"
      },
      "source": [
        "!pip3 install pydicom\n",
        "!pip uninstall -y kaggle\n",
        "!pip install --upgrade pip\n",
        "!pip3 install --upgrade kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/56/342e1f8ce5afe63bf65c23d0b2c1cd5a05600caad1c211c39725d3a4cc56/pydicom-2.0.0-py3-none-any.whl (35.4MB)\n",
            "\u001b[K     |████████████████████████████████| 35.5MB 88kB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.0.0\n",
            "Uninstalling kaggle-1.5.6:\n",
            "  Successfully uninstalled kaggle-1.5.6\n",
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/84/23ed6a1796480a6f1a2d38f2802901d078266bda38388954d01d3f2e821d/pip-20.1.1-py2.py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 7.0MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-20.1.1\n",
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.6.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.6-py3-none-any.whl size=72859 sha256=eb0b5ec7044e8cfce4d13088c572079c93e81ef6fad6b0ba694695151d58ff26\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/3e/ff/77407ebac3ef71a79b9166a8382aecf88415a0bcbe3c095a01\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "Successfully installed kaggle-1.5.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GbR1YhZ41kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /root/.kaggle\n",
        "!echo '{\"username\":\"hector6298\",\"key\":\"724778e3045b27ede8002c9f01b9da72\"}' > /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZYhnB06xOLQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "9bea9331-6ff3-415c-bb89-b2c2edea125e"
      },
      "source": [
        "!git clone https://github.com/ieee8023/covid-chestxray-dataset.git\n",
        "!git clone https://github.com/agchung/Figure1-COVID-chestxray-dataset\n",
        "!git clone https://github.com/agchung/Actualmed-COVID-chestxray-dataset\n",
        "!kaggle datasets download -d \"tawsifurrahman/covid19-radiography-database\"\n",
        "!kaggle competitions download -c \"rsna-pneumonia-detection-challenge\" "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'covid-chestxray-dataset'...\n",
            "remote: Enumerating objects: 3213, done.\u001b[K\n",
            "remote: Total 3213 (delta 0), reused 0 (delta 0), pack-reused 3213\u001b[K\n",
            "Receiving objects: 100% (3213/3213), 582.57 MiB | 8.41 MiB/s, done.\n",
            "Resolving deltas: 100% (1276/1276), done.\n",
            "Checking out files: 100% (991/991), done.\n",
            "Cloning into 'Figure1-COVID-chestxray-dataset'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Counting objects: 100% (112/112), done.\u001b[K\n",
            "remote: Compressing objects: 100% (97/97), done.\u001b[K\n",
            "remote: Total 112 (delta 28), reused 95 (delta 14), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (112/112), 14.13 MiB | 11.32 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n",
            "Cloning into 'Actualmed-COVID-chestxray-dataset'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 422 (delta 1), reused 6 (delta 1), pack-reused 412\u001b[K\n",
            "Receiving objects: 100% (422/422), 1.56 GiB | 22.14 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n",
            "Checking out files: 100% (240/240), done.\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading covid19-radiography-database.zip to /content\n",
            " 99% 1.14G/1.15G [00:21<00:00, 26.7MB/s]\n",
            "100% 1.15G/1.15G [00:21<00:00, 58.4MB/s]\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Downloading rsna-pneumonia-detection-challenge.zip to /content\n",
            " 99% 3.64G/3.66G [01:29<00:00, 57.5MB/s]\n",
            "100% 3.66G/3.66G [01:29<00:00, 43.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvUPlGjQx-mV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip rsna-pneumonia-detection-challenge.zip\n",
        "!rm rsna-pneumonia-detection-challenge.zip\n",
        "!unzip covid19-radiography-database.zip\n",
        "!rm covid19-radiography-database.zip\n",
        "!mkdir data\n",
        "!mkdir data/train\n",
        "!mkdir data/test\n",
        "!mkdir /content/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSkyz7CQ-e5c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "f0177dbf-6406-4015-a711-13d9ba8170bf"
      },
      "source": [
        "!python3 \"/content/drive/My Drive/COVID-Net-master/COVID-Net-master/create_covidx_v3.py\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data distribution from covid datasets:\n",
            "{'normal': 0, 'pneumonia': 40, 'COVID-19': 568}\n",
            "Key:  pneumonia\n",
            "Test patients:  ['8', '31']\n",
            "Key:  COVID-19\n",
            "Test patients:  ['19', '20', '36', '42', '86', '94', '97', '117', '132', '138', '144', '150', '163', '169', '174', '175', '179', '190', '191COVID-00024', 'COVID-00025', 'COVID-00026', 'COVID-00027', 'COVID-00029', 'COVID-00030', 'COVID-00032', 'COVID-00033', 'COVID-00035', 'COVID-00036', 'COVID-00037', 'COVID-00038', 'ANON24', 'ANON45', 'ANON126', 'ANON106', 'ANON67', 'ANON153', 'ANON135', 'ANON44', 'ANON29', 'ANON201', 'ANON191', 'ANON234', 'ANON110', 'ANON112', 'ANON73', 'ANON220', 'ANON189', 'ANON30', 'ANON53', 'ANON46', 'ANON218', 'ANON240', 'ANON100', 'ANON237', 'ANON158', 'ANON174', 'ANON19', 'ANON195', 'COVID-19(119)', 'COVID-19(87)', 'COVID-19(70)', 'COVID-19(94)', 'COVID-19(215)', 'COVID-19(77)', 'COVID-19(213)', 'COVID-19(81)', 'COVID-19(216)', 'COVID-19(72)', 'COVID-19(106)', 'COVID-19(131)', 'COVID-19(107)', 'COVID-19(116)', 'COVID-19(95)', 'COVID-19(214)', 'COVID-19(129)']\n",
            "test count:  {'normal': 0, 'pneumonia': 5, 'COVID-19': 100}\n",
            "train count:  {'normal': 0, 'pneumonia': 35, 'COVID-19': 468}\n",
            "test count:  {'normal': 885, 'pneumonia': 594, 'COVID-19': 100}\n",
            "train count:  {'normal': 7966, 'pneumonia': 5458, 'COVID-19': 468}\n",
            "Final stats\n",
            "Train count:  {'normal': 7966, 'pneumonia': 5458, 'COVID-19': 468}\n",
            "Test count:  {'normal': 885, 'pneumonia': 594, 'COVID-19': 100}\n",
            "Total length of train:  13892\n",
            "Total length of test:  1579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiHdtwQXw6MM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.exposure import equalize_adapthist\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import average_precision_score\n",
        "from tensorflow.python.keras.callbacks import Callback\n",
        "from sklearn.metrics import recall_score, classification_report\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.utils import class_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5DosRcayVMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _process_csv_file(file):\n",
        "    with open(file, 'r') as fr:\n",
        "        files = fr.readlines()\n",
        "    return files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8JtYpk-ICDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def xray_enhance(img):\n",
        "  gaussian_3 = cv2.GaussianBlur(img, (3,3), 10.0)\n",
        "  unsharp_image = cv2.addWeighted(img, 1.5, gaussian_3, -0.5, 0, img)\n",
        "  return unsharp_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LpoyMWxTAu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import compress\n",
        "def clean_annotation(annotations, train_dir, test_dir):\n",
        "  files = []\n",
        "  for r, d, f in os.walk(train_dir):\n",
        "    for file in f:\n",
        "      #print(os.path.join(r, file))\n",
        "      files.append(os.path.join(r, file))\n",
        "  for r, d, f in os.walk(test_dir):\n",
        "    for file in f:\n",
        "      #print(os.path.join(r, file))\n",
        "      files.append(os.path.join(r, file))\n",
        "\n",
        "  mask = np.ones(len(annotations), dtype=bool)\n",
        "\n",
        "  for i in range(len(annotations)):\n",
        "    if f\"{annotations[i][1]}\" not in files:\n",
        "      print(f\"Deleting {annotations[i][1]}\")\n",
        "      mask[i] = False\n",
        "  return list(compress(annotations, mask)), files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb8nU9QU19l1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.001\n",
        "BATCH_SIZE = 64\n",
        "INPUT_SIZE = 224\n",
        "NCLASSES = 3\n",
        "INPUT_SHAPE = (INPUT_SIZE, INPUT_SIZE, NCLASSES)\n",
        "ENHANCE = True\n",
        "TRAINFILE = '/content/train_split_v3.txt'\n",
        "TESTFILE = '/content/test_split_v3.txt'\n",
        "DATADIR = '/content/data'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEpV1yjddFqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataset for fold 1\n",
        "dataset_train = _process_csv_file(TRAINFILE)\n",
        "dataset_test = _process_csv_file(TESTFILE)\n",
        "datasets = {'normal': [], 'pneumonia': [], 'COVID-19': []}\n",
        "for l in dataset_train:\n",
        "  entry = l.split()\n",
        "  entry[1] = f\"data/train/{entry[1]}\"\n",
        "  datasets[entry[2]].append(entry)\n",
        "for l in dataset_test:\n",
        "  entry = l.split()\n",
        "  entry[1] = f\"data/test/{entry[1]}\"\n",
        "  datasets[entry[2]].append(entry)\n",
        "\n",
        "break_point_normal = int(len(datasets['normal'])/5)\n",
        "break_point_covid = int(len(datasets['COVID-19'])/5)\n",
        "break_point_pneumonia = int(len(datasets['pneumonia'])/5)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7sfCosqj75t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = [datasets['normal'][0:(break_point_normal*4)] + datasets['pneumonia'][0:(break_point_pneumonia*4)] + datasets['COVID-19'][0:(break_point_covid*4)]]\n",
        "test_set = [datasets['normal'][(break_point_normal*4):] + datasets['pneumonia'][(break_point_pneumonia*4):] + datasets['COVID-19'][(break_point_covid*4):]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m4FMuRwj_JA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set1 = datasets['normal'][0:(break_point_normal*3)] + datasets['pneumonia'][0:(break_point_pneumonia*3)] + datasets['COVID-19'][0:(break_point_covid*3)]\n",
        "train_set2 = datasets['normal'][(break_point_normal*4):] + datasets['pneumonia'][(break_point_pneumonia*4):] + datasets['COVID-19'][(break_point_covid*4):]\n",
        "train_set = [train_set1 + train_set2]\n",
        "test_set = [datasets['normal'][(break_point_normal*3):(break_point_normal*4)] + datasets['pneumonia'][(break_point_pneumonia*3):(break_point_pneumonia*4)] + datasets['COVID-19'][(break_point_covid*3):(break_point_covid*4)]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cU9PnCavb4HY",
        "colab": {}
      },
      "source": [
        "train_set = [datasets['normal'][(break_point_normal*1):] + datasets['pneumonia'][(break_point_pneumonia*1):] + datasets['COVID-19'][(break_point_covid*1):]]\n",
        "test_set = [datasets['normal'][0:(break_point_normal*1)] + datasets['pneumonia'][0:(break_point_pneumonia*1)] + datasets['COVID-19'][0:(break_point_covid*1)]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBj8KIF3aaan",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b818250-b0f3-45e8-d10f-cb8e2171f638"
      },
      "source": [
        "print(len(test_set[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBNjJDQrTSEz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "6e67b1ca-3cf3-4f0b-8bca-9159f50f4ebd"
      },
      "source": [
        "print(\"go for train...\")\n",
        "annotations_train, filestr = clean_annotation(train_set[0], \"data/train\", \"data/test\")\n",
        "print(\"go for test...\")\n",
        "annotations_test, fileste = clean_annotation(test_set[0], \"data/train\", \"data/test\")\n",
        "print('done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "go for train...\n",
            "go for test...\n",
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MBZnMzQbEPk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2599799d-0ad3-4694-8a59-0160752afc32"
      },
      "source": [
        "print(len(annotations_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12378\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FLfHpEbrHJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def crop_top(img, percent=0.15):\n",
        "    offset = int(img.shape[0] * percent)\n",
        "    return img[offset:]\n",
        "\n",
        "def central_crop(img):\n",
        "    size = min(img.shape[0], img.shape[1])\n",
        "    offset_h = int((img.shape[0] - size) / 2)\n",
        "    offset_w = int((img.shape[1] - size) / 2)\n",
        "    return img[offset_h:offset_h + size, offset_w:offset_w + size]\n",
        "\n",
        "def process_image_file(filepath, top_percent, size, enhance=False):\n",
        "    img = cv2.imread(filepath)\n",
        "    img = crop_top(img, percent=top_percent)\n",
        "    img = central_crop(img)\n",
        "    img = cv2.resize(img, (size, size))\n",
        "    if enhance:\n",
        "      img = xray_enhance(img)\n",
        "    return img\n",
        "\n",
        "_augmentation_transform = ImageDataGenerator(\n",
        "    featurewise_center=False,\n",
        "    featurewise_std_normalization=False,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=(0.9, 1.1),\n",
        "    zoom_range=(0.85, 1.15),\n",
        "    fill_mode='constant',\n",
        "    cval=0.,\n",
        ")\n",
        "\n",
        "def apply_augmentation(img):\n",
        "    img = _augmentation_transform.random_transform(img)\n",
        "    return img\n",
        "\n",
        "class BalanceCovidDataset(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            dataset,\n",
        "            is_training=True,\n",
        "            batch_size=8,\n",
        "            input_shape=(224, 224),\n",
        "            n_classes=3,\n",
        "            num_channels=3,\n",
        "            mapping={\n",
        "                'normal': 0,\n",
        "                'pneumonia': 1,\n",
        "                'COVID-19': 2\n",
        "            },\n",
        "            shuffle=True,\n",
        "            augmentation=apply_augmentation,\n",
        "            covid_percent=0.3,\n",
        "            top_percent=0.08,\n",
        "            balancing=False,\n",
        "            enhance=False,\n",
        "            aug=False\n",
        "    ):\n",
        "        'Initialization'\n",
        "        self.dataset = dataset\n",
        "        self.is_training = is_training\n",
        "        self.batch_size = batch_size\n",
        "        self.N = len(self.dataset)\n",
        "        self.input_shape = input_shape\n",
        "        self.n_classes = n_classes\n",
        "        self.num_channels = num_channels\n",
        "        self.mapping = mapping\n",
        "        self.shuffle = True\n",
        "        self.covid_percent = covid_percent\n",
        "        self.n = 0\n",
        "        self.augmentation = augmentation\n",
        "        self.top_percent = top_percent\n",
        "        self.balancing = balancing\n",
        "        self.mean = None\n",
        "        self.std = None\n",
        "        self.enhance = enhance\n",
        "        self.aug=aug\n",
        "        datasets = {'normal': [], 'pneumonia': [], 'COVID-19': []}\n",
        "        for l in self.dataset:\n",
        "          datasets[l[2]].append(l)\n",
        "        if self.balancing:\n",
        "          self.datasets = [\n",
        "              datasets['normal'] + datasets['pneumonia'],\n",
        "              datasets['COVID-19'],\n",
        "          ]\n",
        "        else:\n",
        "          self.datasets = [\n",
        "              datasets['normal'] + datasets['pneumonia'] + datasets['COVID-19']\n",
        "          ]\n",
        "          #print(len(self.datasets[0]), len(self.datasets[1]))\n",
        "\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __next__(self):\n",
        "        # Get one batch of data\n",
        "        batch_x, batch_y = self.__getitem__(self.n)\n",
        "        # Batch index\n",
        "        self.n += 1\n",
        "\n",
        "        # If we have processed the entire dataset then\n",
        "        if self.n >= self.__len__():\n",
        "            self.on_epoch_end\n",
        "            self.n = 0\n",
        "\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.datasets[0]) / float(self.batch_size)))\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        if self.shuffle == True:\n",
        "            for v in self.datasets:\n",
        "                np.random.shuffle(v)\n",
        "\n",
        "    def include_statistics(mean, std):\n",
        "      self.mean = mean\n",
        "      self.std = std\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x, batch_y = np.zeros(\n",
        "            (self.batch_size, *self.input_shape,\n",
        "             self.num_channels)), np.zeros(self.batch_size)\n",
        "\n",
        "        batch_files = self.datasets[0][idx * self.batch_size:(idx + 1) *\n",
        "                                       self.batch_size]\n",
        "        if self.balancing:\n",
        "       # upsample covid cases\n",
        "          covid_size = max(int(len(batch_files) * self.covid_percent), 1)\n",
        "          covid_inds = np.random.choice(np.arange(len(batch_files)),\n",
        "                                        size=covid_size,\n",
        "                                        replace=False)\n",
        "         \n",
        "          choices_indices = np.random.choice(np.arange(len(self.datasets[1])),\n",
        "                                        size=covid_size,\n",
        "                                        replace=False)\n",
        "          covid_files = [self.datasets[1][i] for i in choices_indices]\n",
        "          for i in range(covid_size):\n",
        "              batch_files[covid_inds[i]] = covid_files[i]\n",
        "\n",
        "        for i in range(len(batch_files)):\n",
        "            sample = batch_files[i]\n",
        "\n",
        "\n",
        "            x = process_image_file(sample[1],\n",
        "                                   self.top_percent,\n",
        "                                   self.input_shape[0], self.enhance)\n",
        "\n",
        "            if self.is_training and hasattr(self, 'augmentation') and self.aug:\n",
        "                x = self.augmentation(x)\n",
        "\n",
        "            x = x.astype('float32') / 255.0\n",
        "            if self.mean is not None and self.std is not None:\n",
        "              x = (x-mean)/std\n",
        "\n",
        "            y = self.mapping[sample[2]]\n",
        "\n",
        "            batch_x[i] = x\n",
        "            batch_y[i] = y\n",
        "\n",
        "        return batch_x, keras.utils.to_categorical(batch_y, num_classes=self.n_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJj25O4k2xbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator = BalanceCovidDataset(\n",
        "                                dataset=annotations_train,\n",
        "                                batch_size=BATCH_SIZE,\n",
        "                                input_shape=(INPUT_SIZE, INPUT_SIZE),\n",
        "                                top_percent=0.08,\n",
        "                                balancing=False,\n",
        "                                enhance=ENHANCE,\n",
        "                                aug=False)\n",
        "\n",
        "test_generator = BalanceCovidDataset(\n",
        "                                dataset=annotations_test,\n",
        "                                batch_size=BATCH_SIZE,\n",
        "                                is_training=False,\n",
        "                                input_shape=(INPUT_SIZE, INPUT_SIZE),\n",
        "                                top_percent=0.08,\n",
        "                                balancing=False,\n",
        "                                enhance=ENHANCE,\n",
        "                                aug=False)\n",
        "                                \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3VpnSwl3iI_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ccf722a0-a0fe-4f7d-ca6d-168d03422bf0"
      },
      "source": [
        "base_model = tf.keras.applications.Xception(\n",
        "    include_top=False, weights='imagenet', input_shape=INPUT_SHAPE,\n",
        ")\n",
        "base_model.trainable = True\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajNtGyCMEZ3A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(base_model, input_shape):\n",
        "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "  global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "  prediction_layer = tf.keras.layers.Dense(NCLASSES, activation='softmax')\n",
        "\n",
        "  x = inputs\n",
        "  x = base_model(x)\n",
        "  x = global_average_layer(x)\n",
        "  x = prediction_layer(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w35fEw_8zBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "METRICS = [\n",
        "  tf.keras.metrics.CategoricalAccuracy(),\n",
        "\n",
        "  keras.metrics.Precision(name='precision_0', class_id=0),\n",
        "  keras.metrics.Recall(name='recall_0', class_id=0),\n",
        "  keras.metrics.AUC(name='auc_all', multi_label=True),\n",
        "\n",
        "  keras.metrics.Precision(name='precision_1', class_id=1),\n",
        "  keras.metrics.Recall(name='recall_1', class_id=1),\n",
        "\n",
        "  keras.metrics.Precision(name='precision_2', class_id=2),\n",
        "  keras.metrics.Recall(name='recall_2', class_id=2),\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYlsTTptE_40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics = METRICS\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "model = get_model(base_model, INPUT_SHAPE)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss_object,\n",
        "              metrics = metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkAHdGweSGDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scheduler(epoch):\n",
        "  if epoch <= 5:\n",
        "    return 0.005\n",
        "  elif epoch > 5 and epoch <= 10:\n",
        "    return 0.001\n",
        "  else:\n",
        "    return 0.001 * tf.math.exp(0.1 * (10 - epoch))\n",
        "scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"/content/logs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YwrGaX7TDPi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "outputId": "ddcf3aba-7599-4e5f-8cb6-67367c57cbc5"
      },
      "source": [
        "model_history = model.fit(x=train_generator, \n",
        "                epochs=EPOCHS,\n",
        "                validation_data=test_generator,\n",
        "                verbose=1,\n",
        "                callbacks=[scheduler_callback, tensorboard_callback]\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "194/194 [==============================] - 302s 2s/step - loss: 0.3650 - categorical_accuracy: 0.8672 - precision_0: 0.9053 - recall_0: 0.8954 - auc_all: 0.9302 - precision_1: 0.8800 - recall_1: 0.8015 - precision_2: 0.8146 - recall_2: 0.2703 - val_loss: 1.3077 - val_categorical_accuracy: 0.8017 - val_precision_0: 0.8935 - val_recall_0: 0.7959 - val_auc_all: 0.7625 - val_precision_1: 0.7041 - val_recall_1: 0.8851 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - lr: 0.0050\n",
            "Epoch 2/20\n",
            "194/194 [==============================] - 295s 2s/step - loss: 0.2293 - categorical_accuracy: 0.9175 - precision_0: 0.9299 - recall_0: 0.9463 - auc_all: 0.9720 - precision_1: 0.9218 - recall_1: 0.8860 - precision_2: 0.8793 - recall_2: 0.6725 - val_loss: 1.6119 - val_categorical_accuracy: 0.7905 - val_precision_0: 0.7771 - val_recall_0: 0.9923 - val_auc_all: 0.8724 - val_precision_1: 0.9815 - val_recall_1: 0.4835 - val_precision_2: 0.3818 - val_recall_2: 0.5575 - lr: 0.0050\n",
            "Epoch 3/20\n",
            "194/194 [==============================] - 292s 2s/step - loss: 0.2069 - categorical_accuracy: 0.9242 - precision_0: 0.9349 - recall_0: 0.9501 - auc_all: 0.9759 - precision_1: 0.9284 - recall_1: 0.8951 - precision_2: 0.8904 - recall_2: 0.7319 - val_loss: 2.4403 - val_categorical_accuracy: 0.6920 - val_precision_0: 0.6682 - val_recall_0: 0.9763 - val_auc_all: 0.8515 - val_precision_1: 0.9971 - val_recall_1: 0.2810 - val_precision_2: 0.4184 - val_recall_2: 0.5221 - lr: 0.0050\n",
            "Epoch 4/20\n",
            "194/194 [==============================] - 283s 1s/step - loss: 0.1759 - categorical_accuracy: 0.9344 - precision_0: 0.9451 - recall_0: 0.9520 - auc_all: 0.9831 - precision_1: 0.9363 - recall_1: 0.9162 - precision_2: 0.9070 - recall_2: 0.7714 - val_loss: 0.6756 - val_categorical_accuracy: 0.8473 - val_precision_0: 0.8389 - val_recall_0: 0.9504 - val_auc_all: 0.7894 - val_precision_1: 0.8632 - val_recall_1: 0.7719 - val_precision_2: 0.0000e+00 - val_recall_2: 0.0000e+00 - lr: 0.0050\n",
            "Epoch 5/20\n",
            "194/194 [==============================] - 278s 1s/step - loss: 0.1533 - categorical_accuracy: 0.9431 - precision_0: 0.9534 - recall_0: 0.9602 - auc_all: 0.9874 - precision_1: 0.9457 - recall_1: 0.9252 - precision_2: 0.8786 - recall_2: 0.7956 - val_loss: 1.0242 - val_categorical_accuracy: 0.8638 - val_precision_0: 0.8501 - val_recall_0: 0.9757 - val_auc_all: 0.7908 - val_precision_1: 0.8909 - val_recall_1: 0.7760 - val_precision_2: 1.0000 - val_recall_2: 0.0088 - lr: 0.0050\n",
            "Epoch 6/20\n",
            "194/194 [==============================] - 279s 1s/step - loss: 0.1399 - categorical_accuracy: 0.9499 - precision_0: 0.9591 - recall_0: 0.9647 - auc_all: 0.9901 - precision_1: 0.9511 - recall_1: 0.9364 - precision_2: 0.9073 - recall_2: 0.8176 - val_loss: 0.4945 - val_categorical_accuracy: 0.8361 - val_precision_0: 0.8995 - val_recall_0: 0.8593 - val_auc_all: 0.9463 - val_precision_1: 0.9360 - val_recall_1: 0.7612 - val_precision_2: 0.3065 - val_recall_2: 0.8761 - lr: 0.0050\n",
            "Epoch 7/20\n",
            "194/194 [==============================] - 275s 1s/step - loss: 0.0600 - categorical_accuracy: 0.9797 - precision_0: 0.9816 - recall_0: 0.9803 - auc_all: 0.9985 - precision_1: 0.9790 - recall_1: 0.9734 - precision_2: 0.9749 - recall_2: 0.9407 - val_loss: 0.3134 - val_categorical_accuracy: 0.9011 - val_precision_0: 0.9243 - val_recall_0: 0.9360 - val_auc_all: 0.9651 - val_precision_1: 0.9260 - val_recall_1: 0.8587 - val_precision_2: 0.7143 - val_recall_2: 0.7522 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "194/194 [==============================] - 269s 1s/step - loss: 0.0205 - categorical_accuracy: 0.9925 - precision_0: 0.9968 - recall_0: 0.9909 - auc_all: 0.9999 - precision_1: 0.9865 - recall_1: 0.9952 - precision_2: 0.9955 - recall_2: 0.9824 - val_loss: 0.4379 - val_categorical_accuracy: 0.9139 - val_precision_0: 0.9283 - val_recall_0: 0.9504 - val_auc_all: 0.9356 - val_precision_1: 0.9003 - val_recall_1: 0.8876 - val_precision_2: 0.8095 - val_recall_2: 0.6018 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "194/194 [==============================] - 266s 1s/step - loss: 0.0120 - categorical_accuracy: 0.9971 - precision_0: 0.9980 - recall_0: 0.9972 - auc_all: 0.9999 - precision_1: 0.9961 - recall_1: 0.9967 - precision_2: 0.9956 - recall_2: 0.9978 - val_loss: 0.4460 - val_categorical_accuracy: 0.9056 - val_precision_0: 0.9429 - val_recall_0: 0.9200 - val_auc_all: 0.9456 - val_precision_1: 0.8676 - val_recall_1: 0.9041 - val_precision_2: 0.7500 - val_recall_2: 0.6903 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "194/194 [==============================] - 265s 1s/step - loss: 0.0059 - categorical_accuracy: 0.9994 - precision_0: 0.9994 - recall_0: 0.9997 - auc_all: 1.0000 - precision_1: 0.9994 - recall_1: 0.9988 - precision_2: 0.9978 - recall_2: 0.9978 - val_loss: 0.5402 - val_categorical_accuracy: 0.9027 - val_precision_0: 0.9232 - val_recall_0: 0.9344 - val_auc_all: 0.9327 - val_precision_1: 0.8788 - val_recall_1: 0.8810 - val_precision_2: 0.8140 - val_recall_2: 0.6195 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "194/194 [==============================] - 262s 1s/step - loss: 0.0132 - categorical_accuracy: 0.9961 - precision_0: 0.9963 - recall_0: 0.9968 - auc_all: 0.9998 - precision_1: 0.9955 - recall_1: 0.9952 - precision_2: 0.9978 - recall_2: 0.9934 - val_loss: 0.6225 - val_categorical_accuracy: 0.9078 - val_precision_0: 0.9083 - val_recall_0: 0.9724 - val_auc_all: 0.9501 - val_precision_1: 0.9491 - val_recall_1: 0.8174 - val_precision_2: 0.6309 - val_recall_2: 0.8319 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "194/194 [==============================] - 260s 1s/step - loss: 0.0090 - categorical_accuracy: 0.9977 - precision_0: 0.9986 - recall_0: 0.9985 - auc_all: 0.9995 - precision_1: 0.9975 - recall_1: 0.9973 - precision_2: 0.9847 - recall_2: 0.9890 - val_loss: 0.5319 - val_categorical_accuracy: 0.9018 - val_precision_0: 0.9200 - val_recall_0: 0.9393 - val_auc_all: 0.9425 - val_precision_1: 0.8927 - val_recall_1: 0.8595 - val_precision_2: 0.7203 - val_recall_2: 0.7522 - lr: 9.0484e-04\n",
            "Epoch 13/20\n",
            "194/194 [==============================] - 259s 1s/step - loss: 0.0050 - categorical_accuracy: 0.9989 - precision_0: 0.9997 - recall_0: 0.9993 - auc_all: 1.0000 - precision_1: 0.9990 - recall_1: 0.9990 - precision_2: 0.9869 - recall_2: 0.9912 - val_loss: 0.5620 - val_categorical_accuracy: 0.8980 - val_precision_0: 0.9162 - val_recall_0: 0.9404 - val_auc_all: 0.9472 - val_precision_1: 0.8979 - val_recall_1: 0.8430 - val_precision_2: 0.6642 - val_recall_2: 0.8053 - lr: 8.1873e-04\n",
            "Epoch 14/20\n",
            "194/194 [==============================] - 265s 1s/step - loss: 0.0048 - categorical_accuracy: 0.9987 - precision_0: 0.9987 - recall_0: 0.9992 - auc_all: 1.0000 - precision_1: 0.9988 - recall_1: 0.9981 - precision_2: 0.9978 - recall_2: 0.9978 - val_loss: 0.5698 - val_categorical_accuracy: 0.9155 - val_precision_0: 0.9372 - val_recall_0: 0.9459 - val_auc_all: 0.9453 - val_precision_1: 0.9024 - val_recall_1: 0.8860 - val_precision_2: 0.7119 - val_recall_2: 0.7434 - lr: 7.4082e-04\n",
            "Epoch 15/20\n",
            "194/194 [==============================] - 260s 1s/step - loss: 0.0036 - categorical_accuracy: 0.9994 - precision_0: 0.9996 - recall_0: 0.9993 - auc_all: 1.0000 - precision_1: 0.9990 - recall_1: 0.9994 - precision_2: 1.0000 - recall_2: 1.0000 - val_loss: 0.5826 - val_categorical_accuracy: 0.9129 - val_precision_0: 0.9197 - val_recall_0: 0.9597 - val_auc_all: 0.9416 - val_precision_1: 0.9218 - val_recall_1: 0.8579 - val_precision_2: 0.7203 - val_recall_2: 0.7522 - lr: 6.7032e-04\n",
            "Epoch 16/20\n",
            "194/194 [==============================] - 262s 1s/step - loss: 0.0017 - categorical_accuracy: 1.0000 - precision_0: 1.0000 - recall_0: 1.0000 - auc_all: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - precision_2: 1.0000 - recall_2: 1.0000 - val_loss: 0.5929 - val_categorical_accuracy: 0.9168 - val_precision_0: 0.9241 - val_recall_0: 0.9597 - val_auc_all: 0.9447 - val_precision_1: 0.9272 - val_recall_1: 0.8628 - val_precision_2: 0.7165 - val_recall_2: 0.8053 - lr: 6.0653e-04\n",
            "Epoch 17/20\n",
            "194/194 [==============================] - 256s 1s/step - loss: 0.0017 - categorical_accuracy: 0.9998 - precision_0: 0.9997 - recall_0: 0.9999 - auc_all: 1.0000 - precision_1: 0.9998 - recall_1: 0.9996 - precision_2: 1.0000 - recall_2: 1.0000 - val_loss: 0.6493 - val_categorical_accuracy: 0.9050 - val_precision_0: 0.9494 - val_recall_0: 0.9106 - val_auc_all: 0.9402 - val_precision_1: 0.8591 - val_recall_1: 0.9124 - val_precision_2: 0.7477 - val_recall_2: 0.7345 - lr: 5.4881e-04\n",
            "Epoch 18/20\n",
            "194/194 [==============================] - 251s 1s/step - loss: 0.0037 - categorical_accuracy: 0.9991 - precision_0: 0.9992 - recall_0: 0.9996 - auc_all: 0.9999 - precision_1: 0.9994 - recall_1: 0.9988 - precision_2: 0.9956 - recall_2: 0.9956 - val_loss: 0.7691 - val_categorical_accuracy: 0.9098 - val_precision_0: 0.9248 - val_recall_0: 0.9636 - val_auc_all: 0.9526 - val_precision_1: 0.9438 - val_recall_1: 0.8322 - val_precision_2: 0.5593 - val_recall_2: 0.8761 - lr: 4.9659e-04\n",
            "Epoch 19/20\n",
            "194/194 [==============================] - 252s 1s/step - loss: 0.0012 - categorical_accuracy: 1.0000 - precision_0: 1.0000 - recall_0: 1.0000 - auc_all: 1.0000 - precision_1: 1.0000 - recall_1: 1.0000 - precision_2: 1.0000 - recall_2: 1.0000 - val_loss: 0.6381 - val_categorical_accuracy: 0.9206 - val_precision_0: 0.9291 - val_recall_0: 0.9608 - val_auc_all: 0.9301 - val_precision_1: 0.9170 - val_recall_1: 0.8851 - val_precision_2: 0.7935 - val_recall_2: 0.6460 - lr: 4.4933e-04\n",
            "Epoch 20/20\n",
            "194/194 [==============================] - 253s 1s/step - loss: 0.0013 - categorical_accuracy: 0.9998 - precision_0: 1.0000 - recall_0: 0.9996 - auc_all: 1.0000 - precision_1: 0.9994 - recall_1: 1.0000 - precision_2: 1.0000 - recall_2: 1.0000 - val_loss: 0.6680 - val_categorical_accuracy: 0.9209 - val_precision_0: 0.9279 - val_recall_0: 0.9647 - val_auc_all: 0.9381 - val_precision_1: 0.9324 - val_recall_1: 0.8669 - val_precision_2: 0.7177 - val_recall_2: 0.7876 - lr: 4.0657e-04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B30_WM2_EVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def test_step(test_images, test_labels):\n",
        "  return model(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8sjLKDO7o_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "for j in range(len(test_generator)):\n",
        "\n",
        "    test_images, test_labels = next(test_generator)\n",
        "    #print(test_labels)\n",
        "    \n",
        "    #imgs += len(test_images)\n",
        "    predictions = test_step(test_images, test_labels)\n",
        "    predictions_classnum = np.argmax(predictions, axis=1)\n",
        "    test_labels_ = np.argmax(test_labels, axis=1)\n",
        "    confidences = np.amax(predictions)\n",
        "\n",
        "    for item in range(BATCH_SIZE):\n",
        "        y_pred.append(predictions_classnum[item])\n",
        "        y_true.append(int(test_labels_[item]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CsOCDEH_29Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "d984d219-2a0d-4ded-8954-97d295ffc884"
      },
      "source": [
        "target_names = ['Normal', 'Pneumonia', 'COVID-19']\n",
        "print(classification_report(y_true, y_pred, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.93      0.96      0.95      1813\n",
            "   Pneumonia       0.93      0.87      0.90      1210\n",
            "    COVID-19       0.72      0.79      0.75       113\n",
            "\n",
            "    accuracy                           0.92      3136\n",
            "   macro avg       0.86      0.87      0.87      3136\n",
            "weighted avg       0.92      0.92      0.92      3136\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ycXREG6Tnpj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "166cfc60-bac3-4739-cf43-85c309602e62"
      },
      "source": [
        "\n",
        "model.save_weights('covid_net_weights_IENBF5.h5')\n",
        "files.download('covid_net_weights_IENBF5.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_af91ad17-2d4d-4b21-976a-aa558e9dea5e\", \"covid_net_weights_IENBF5.h5\", 83648776)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0g_hKSHUoLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r /content/logsRB.zip /content/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY93iIMKVBUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(\"/content/logsRB.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}